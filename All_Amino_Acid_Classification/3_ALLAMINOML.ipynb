{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86170217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 13:28:31.926859: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-03 13:28:32.537986: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-03 13:28:32.538035: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-03 13:28:32.538039: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.layers import Dense, Conv1D, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.backend import dropout\n",
    "import keras\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc998188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed value\n",
    "seed_value= 3\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "tf.keras.utils.set_random_seed(seed_value)\n",
    "\n",
    "# 5. For layers that introduce randomness like dropout, make sure to set seed values \n",
    "# model.add(Dropout(0.25, seed=seed_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcaa9906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant datasets\n",
    "X_train = pd.read_csv(\"finalXtrain.csv\")\n",
    "y_train = pd.read_csv(\"finalYtrain.csv\")\n",
    "X_test = pd.read_csv(\"finalXtest.csv\")\n",
    "y_test = pd.read_csv(\"finalYtest.csv\")\n",
    "X_val = pd.read_csv(\"finalXval.csv\")\n",
    "y_val = pd.read_csv(\"finalYval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c978f846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(439, 72)\n",
      "(439, 17)\n",
      "(94, 72)\n",
      "(94, 17)\n",
      "(95, 72)\n",
      "(95, 17)\n"
     ]
    }
   ],
   "source": [
    "#check shape\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0066d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model\n",
    "\n",
    "def define_model(seedval):\n",
    "    inputs = tf.keras.Input(shape=(72, 1), name='input')\n",
    "    x = Conv1D(filters=15, kernel_size=3,strides =3)(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
    "    x = Conv1D(filters=30, kernel_size=5,strides =2)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5,seed=seedval)(x)\n",
    "    x = Dense(20,activation ='relu')(x)\n",
    "    x = Dense(10,activation ='relu')(x)\n",
    "    x = Dense(5,activation ='relu')(x)\n",
    "    outputs = Dense(17, activation='softmax', name='predictions')(x)\n",
    "    cnn_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    cnn_model.compile(loss='categorical_crossentropy', optimizer= Adam(), metrics=['accuracy'])\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc3ef548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#earlystopping\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=100)\n",
    "checkpoint_path = 'alllclassweights_march3spring24.h5'\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                   monitor='val_accuracy',\n",
    "                                   save_best_only=True,\n",
    "                                   mode='max',\n",
    "                                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ea70f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 72, 1)]           0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 24, 15)            60        \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 24, 15)           60        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 24, 15)            0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 12, 15)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 4, 30)             2280      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 4, 30)            120       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 4, 30)             0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 2, 30)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 60)                0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 60)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                1220      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 17)                102       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,107\n",
      "Trainable params: 4,017\n",
      "Non-trainable params: 90\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 13:28:33.014485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-03 13:28:33.014677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-03 13:28:33.019880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-03 13:28:33.020085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-03 13:28:33.020245: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-03 13:28:33.020399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-03 13:28:33.020850: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-03 13:28:33.350584: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-03 13:28:33.350882: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-03 13:28:33.351140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-03 13:28:33.351600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-03 13:28:33.351861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-03 13:28:33.352102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-03 13:28:33.958265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-03 13:28:33.958492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-03 13:28:33.958668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-03 13:28:33.958829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-03 13:28:33.958984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-03 13:28:33.959124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46434 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:2e:00.0, compute capability: 8.6\n",
      "2024-03-03 13:28:33.959485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-03 13:28:33.959624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46697 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:41:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = define_model(seed_value)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "783b0079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 13:28:36.076571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8500\n",
      "2024-03-03 13:28:36.251193: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-03 13:28:36.290139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-03-03 13:28:36.291845: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f08f838fe50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-03 13:28:36.291857: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2024-03-03 13:28:36.291861: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2024-03-03 13:28:36.295714: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-03 13:28:36.354274: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-03 13:28:36.392214: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/88 [=========================>....] - ETA: 0s - loss: 47.7188 - accuracy: 0.0557 \n",
      "Epoch 1: val_accuracy improved from -inf to 0.13684, saving model to alllclassweights_march3spring24.h5\n",
      "88/88 [==============================] - 4s 7ms/step - loss: 48.8189 - accuracy: 0.0501 - val_loss: 2.8315 - val_accuracy: 0.1368\n",
      "Epoch 2/400\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 48.6370 - accuracy: 0.0847\n",
      "Epoch 2: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 48.3083 - accuracy: 0.0866 - val_loss: 2.8326 - val_accuracy: 0.1368\n",
      "Epoch 3/400\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 48.3921 - accuracy: 0.0941\n",
      "Epoch 3: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 48.1052 - accuracy: 0.0934 - val_loss: 2.8344 - val_accuracy: 0.0632\n",
      "Epoch 4/400\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 48.0262 - accuracy: 0.0762\n",
      "Epoch 4: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 48.2592 - accuracy: 0.0752 - val_loss: 2.8317 - val_accuracy: 0.0737\n",
      "Epoch 5/400\n",
      "86/88 [============================>.] - ETA: 0s - loss: 47.9953 - accuracy: 0.0953\n",
      "Epoch 5: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 48.0548 - accuracy: 0.0979 - val_loss: 2.8386 - val_accuracy: 0.0526\n",
      "Epoch 6/400\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 47.6959 - accuracy: 0.1026\n",
      "Epoch 6: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 48.1387 - accuracy: 0.0979 - val_loss: 2.8357 - val_accuracy: 0.0947\n",
      "Epoch 7/400\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 48.4646 - accuracy: 0.1000\n",
      "Epoch 7: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 48.1654 - accuracy: 0.1002 - val_loss: 2.8513 - val_accuracy: 0.0737\n",
      "Epoch 8/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 47.9023 - accuracy: 0.0661\n",
      "Epoch 8: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 47.9023 - accuracy: 0.0661 - val_loss: 2.8340 - val_accuracy: 0.0421\n",
      "Epoch 9/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 48.2552 - accuracy: 0.1025\n",
      "Epoch 9: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 48.2552 - accuracy: 0.1025 - val_loss: 2.8405 - val_accuracy: 0.0737\n",
      "Epoch 10/400\n",
      "86/88 [============================>.] - ETA: 0s - loss: 48.2193 - accuracy: 0.0698\n",
      "Epoch 10: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 48.2521 - accuracy: 0.0683 - val_loss: 2.8380 - val_accuracy: 0.1053\n",
      "Epoch 11/400\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 48.3215 - accuracy: 0.1024\n",
      "Epoch 11: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 48.0966 - accuracy: 0.1002 - val_loss: 2.8362 - val_accuracy: 0.0947\n",
      "Epoch 12/400\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 48.5741 - accuracy: 0.0714\n",
      "Epoch 12: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 47.9334 - accuracy: 0.0752 - val_loss: 2.8332 - val_accuracy: 0.0526\n",
      "Epoch 13/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 47.1600 - accuracy: 0.0720\n",
      "Epoch 13: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 47.7473 - accuracy: 0.0661 - val_loss: 2.8283 - val_accuracy: 0.0842\n",
      "Epoch 14/400\n",
      "86/88 [============================>.] - ETA: 0s - loss: 47.2182 - accuracy: 0.0674\n",
      "Epoch 14: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 47.5647 - accuracy: 0.0706 - val_loss: 2.8285 - val_accuracy: 0.1158\n",
      "Epoch 15/400\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 48.3393 - accuracy: 0.0824\n",
      "Epoch 15: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 47.8534 - accuracy: 0.0820 - val_loss: 2.8258 - val_accuracy: 0.1158\n",
      "Epoch 16/400\n",
      "87/88 [============================>.] - ETA: 0s - loss: 47.9206 - accuracy: 0.0851\n",
      "Epoch 16: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 47.7380 - accuracy: 0.0866 - val_loss: 2.8159 - val_accuracy: 0.1053\n",
      "Epoch 17/400\n",
      "86/88 [============================>.] - ETA: 0s - loss: 46.8309 - accuracy: 0.0930\n",
      "Epoch 17: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 47.2478 - accuracy: 0.0911 - val_loss: 2.8160 - val_accuracy: 0.1158\n",
      "Epoch 18/400\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 47.9368 - accuracy: 0.1026\n",
      "Epoch 18: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 47.6167 - accuracy: 0.0957 - val_loss: 2.8211 - val_accuracy: 0.0421\n",
      "Epoch 19/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 47.4435 - accuracy: 0.0880\n",
      "Epoch 19: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 47.5650 - accuracy: 0.0888 - val_loss: 2.8036 - val_accuracy: 0.0947\n",
      "Epoch 20/400\n",
      "86/88 [============================>.] - ETA: 0s - loss: 47.3520 - accuracy: 0.0953\n",
      "Epoch 20: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 47.4948 - accuracy: 0.0957 - val_loss: 2.8084 - val_accuracy: 0.1158\n",
      "Epoch 21/400\n",
      "86/88 [============================>.] - ETA: 0s - loss: 47.3439 - accuracy: 0.0953\n",
      "Epoch 21: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 47.3807 - accuracy: 0.0934 - val_loss: 2.8186 - val_accuracy: 0.1263\n",
      "Epoch 22/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 46.8714 - accuracy: 0.0866\n",
      "Epoch 22: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 46.8714 - accuracy: 0.0866 - val_loss: 2.8351 - val_accuracy: 0.0737\n",
      "Epoch 23/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 46.9754 - accuracy: 0.0933\n",
      "Epoch 23: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 47.0896 - accuracy: 0.0843 - val_loss: 2.8554 - val_accuracy: 0.0526\n",
      "Epoch 24/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 46.3862 - accuracy: 0.1048\n",
      "Epoch 24: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 46.3862 - accuracy: 0.1048 - val_loss: 2.8314 - val_accuracy: 0.0842\n",
      "Epoch 25/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 46.0796 - accuracy: 0.1185\n",
      "Epoch 25: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 46.0796 - accuracy: 0.1185 - val_loss: 2.8306 - val_accuracy: 0.0842\n",
      "Epoch 26/400\n",
      "86/88 [============================>.] - ETA: 0s - loss: 46.1710 - accuracy: 0.1419\n",
      "Epoch 26: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 46.0093 - accuracy: 0.1412 - val_loss: 2.8142 - val_accuracy: 0.0947\n",
      "Epoch 27/400\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 45.4775 - accuracy: 0.1184\n",
      "Epoch 27: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 46.0335 - accuracy: 0.1253 - val_loss: 2.8019 - val_accuracy: 0.0737\n",
      "Epoch 28/400\n",
      "78/88 [=========================>....] - ETA: 0s - loss: 45.4984 - accuracy: 0.1154\n",
      "Epoch 28: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 45.9547 - accuracy: 0.1185 - val_loss: 2.7876 - val_accuracy: 0.0947\n",
      "Epoch 29/400\n",
      "87/88 [============================>.] - ETA: 0s - loss: 45.7175 - accuracy: 0.1103\n",
      "Epoch 29: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 45.8375 - accuracy: 0.1093 - val_loss: 2.8090 - val_accuracy: 0.0947\n",
      "Epoch 30/400\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 45.8189 - accuracy: 0.1459\n",
      "Epoch 30: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 45.1884 - accuracy: 0.1412 - val_loss: 2.7762 - val_accuracy: 0.1368\n",
      "Epoch 31/400\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 44.9394 - accuracy: 0.1737\n",
      "Epoch 31: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 44.9195 - accuracy: 0.1708 - val_loss: 2.8231 - val_accuracy: 0.0947\n",
      "Epoch 32/400\n",
      "87/88 [============================>.] - ETA: 0s - loss: 45.4787 - accuracy: 0.1494\n",
      "Epoch 32: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 45.3086 - accuracy: 0.1481 - val_loss: 2.7787 - val_accuracy: 0.0947\n",
      "Epoch 33/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 45.2712 - accuracy: 0.1276\n",
      "Epoch 33: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 45.2712 - accuracy: 0.1276 - val_loss: 2.7841 - val_accuracy: 0.1368\n",
      "Epoch 34/400\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 43.8559 - accuracy: 0.1211\n",
      "Epoch 34: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 44.6099 - accuracy: 0.1162 - val_loss: 2.7949 - val_accuracy: 0.1053\n",
      "Epoch 35/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 44.2252 - accuracy: 0.1526\n",
      "Epoch 35: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 44.2252 - accuracy: 0.1526 - val_loss: 2.8249 - val_accuracy: 0.0947\n",
      "Epoch 36/400\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 45.3438 - accuracy: 0.1757\n",
      "Epoch 36: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 43.8313 - accuracy: 0.1686 - val_loss: 2.8794 - val_accuracy: 0.0737\n",
      "Epoch 37/400\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 43.5706 - accuracy: 0.1658\n",
      "Epoch 37: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 43.5858 - accuracy: 0.1686 - val_loss: 2.8206 - val_accuracy: 0.1053\n",
      "Epoch 38/400\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 43.2981 - accuracy: 0.1429\n",
      "Epoch 38: val_accuracy did not improve from 0.13684\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 44.1116 - accuracy: 0.1321 - val_loss: 2.8247 - val_accuracy: 0.1368\n",
      "Epoch 39/400\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 44.4707 - accuracy: 0.1435\n",
      "Epoch 39: val_accuracy improved from 0.13684 to 0.14737, saving model to alllclassweights_march3spring24.h5\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 44.1106 - accuracy: 0.1412 - val_loss: 2.7961 - val_accuracy: 0.1474\n",
      "Epoch 40/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 43.6738 - accuracy: 0.1572\n",
      "Epoch 40: val_accuracy did not improve from 0.14737\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 43.6738 - accuracy: 0.1572 - val_loss: 2.8499 - val_accuracy: 0.0947\n",
      "Epoch 41/400\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 42.8210 - accuracy: 0.1388\n",
      "Epoch 41: val_accuracy did not improve from 0.14737\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 43.4144 - accuracy: 0.1367 - val_loss: 2.8480 - val_accuracy: 0.0737\n",
      "Epoch 42/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 43.1463 - accuracy: 0.1595\n",
      "Epoch 42: val_accuracy improved from 0.14737 to 0.15789, saving model to alllclassweights_march3spring24.h5\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 43.1463 - accuracy: 0.1595 - val_loss: 2.7475 - val_accuracy: 0.1579\n",
      "Epoch 43/400\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 42.4427 - accuracy: 0.1447\n",
      "Epoch 43: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 42.2872 - accuracy: 0.1435 - val_loss: 2.7923 - val_accuracy: 0.1368\n",
      "Epoch 44/400\n",
      "81/88 [==========================>...] - ETA: 0s - loss: 42.3955 - accuracy: 0.1580\n",
      "Epoch 44: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 42.6317 - accuracy: 0.1549 - val_loss: 2.7890 - val_accuracy: 0.1263\n",
      "Epoch 45/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 42.2743 - accuracy: 0.1595\n",
      "Epoch 45: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 42.2743 - accuracy: 0.1595 - val_loss: 2.7925 - val_accuracy: 0.0947\n",
      "Epoch 46/400\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 40.3783 - accuracy: 0.1459\n",
      "Epoch 46: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 42.2049 - accuracy: 0.1572 - val_loss: 2.8015 - val_accuracy: 0.1053\n",
      "Epoch 47/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 43.1852 - accuracy: 0.1298\n",
      "Epoch 47: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 43.1852 - accuracy: 0.1298 - val_loss: 2.8360 - val_accuracy: 0.0842\n",
      "Epoch 48/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 42.3453 - accuracy: 0.1307\n",
      "Epoch 48: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 42.1268 - accuracy: 0.1321 - val_loss: 2.7798 - val_accuracy: 0.1053\n",
      "Epoch 49/400\n",
      "79/88 [=========================>....] - ETA: 0s - loss: 41.9787 - accuracy: 0.1772\n",
      "Epoch 49: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 41.2013 - accuracy: 0.1800 - val_loss: 2.8158 - val_accuracy: 0.0947\n",
      "Epoch 50/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 40.9259 - accuracy: 0.1731\n",
      "Epoch 50: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 40.9259 - accuracy: 0.1731 - val_loss: 2.8021 - val_accuracy: 0.0737\n",
      "Epoch 51/400\n",
      "87/88 [============================>.] - ETA: 0s - loss: 42.1732 - accuracy: 0.1402\n",
      "Epoch 51: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 41.9705 - accuracy: 0.1390 - val_loss: 2.7609 - val_accuracy: 0.0842\n",
      "Epoch 52/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 42.6191 - accuracy: 0.1707\n",
      "Epoch 52: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 41.9389 - accuracy: 0.1754 - val_loss: 2.7762 - val_accuracy: 0.0842\n",
      "Epoch 53/400\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 39.5574 - accuracy: 0.1730\n",
      "Epoch 53: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 41.7808 - accuracy: 0.1663 - val_loss: 2.7754 - val_accuracy: 0.0632\n",
      "Epoch 54/400\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 41.6971 - accuracy: 0.1429\n",
      "Epoch 54: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 41.7246 - accuracy: 0.1390 - val_loss: 2.7785 - val_accuracy: 0.0737\n",
      "Epoch 55/400\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 41.8800 - accuracy: 0.1667\n",
      "Epoch 55: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 42.0333 - accuracy: 0.1686 - val_loss: 2.8065 - val_accuracy: 0.0632\n",
      "Epoch 56/400\n",
      "86/88 [============================>.] - ETA: 0s - loss: 40.6697 - accuracy: 0.1535\n",
      "Epoch 56: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 40.5866 - accuracy: 0.1572 - val_loss: 2.8003 - val_accuracy: 0.0737\n",
      "Epoch 57/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 41.6634 - accuracy: 0.1435\n",
      "Epoch 57: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 41.6634 - accuracy: 0.1435 - val_loss: 2.8454 - val_accuracy: 0.0526\n",
      "Epoch 58/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 41.7510 - accuracy: 0.1387\n",
      "Epoch 58: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 41.5953 - accuracy: 0.1321 - val_loss: 2.7836 - val_accuracy: 0.0632\n",
      "Epoch 59/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/88 [============================>.] - ETA: 0s - loss: 40.7146 - accuracy: 0.1651\n",
      "Epoch 59: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 40.5891 - accuracy: 0.1640 - val_loss: 2.8119 - val_accuracy: 0.0632\n",
      "Epoch 60/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 39.2510 - accuracy: 0.1754\n",
      "Epoch 60: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 39.2510 - accuracy: 0.1754 - val_loss: 2.7739 - val_accuracy: 0.0632\n",
      "Epoch 61/400\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 40.9253 - accuracy: 0.1452\n",
      "Epoch 61: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 40.9761 - accuracy: 0.1435 - val_loss: 2.7747 - val_accuracy: 0.0947\n",
      "Epoch 62/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 40.2413 - accuracy: 0.1754\n",
      "Epoch 62: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 40.2413 - accuracy: 0.1754 - val_loss: 2.7307 - val_accuracy: 0.0737\n",
      "Epoch 63/400\n",
      "87/88 [============================>.] - ETA: 0s - loss: 38.9337 - accuracy: 0.1264\n",
      "Epoch 63: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 39.1607 - accuracy: 0.1276 - val_loss: 2.8232 - val_accuracy: 0.0737\n",
      "Epoch 64/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 38.3862 - accuracy: 0.1520\n",
      "Epoch 64: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 38.2595 - accuracy: 0.1458 - val_loss: 2.8427 - val_accuracy: 0.0947\n",
      "Epoch 65/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 40.4589 - accuracy: 0.1787\n",
      "Epoch 65: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 40.4563 - accuracy: 0.1617 - val_loss: 2.8267 - val_accuracy: 0.0737\n",
      "Epoch 66/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 41.3770 - accuracy: 0.1227\n",
      "Epoch 66: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 40.2090 - accuracy: 0.1298 - val_loss: 2.8096 - val_accuracy: 0.0632\n",
      "Epoch 67/400\n",
      "87/88 [============================>.] - ETA: 0s - loss: 39.9689 - accuracy: 0.1264\n",
      "Epoch 67: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 39.9076 - accuracy: 0.1253 - val_loss: 2.7918 - val_accuracy: 0.0526\n",
      "Epoch 68/400\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 38.8486 - accuracy: 0.1718\n",
      "Epoch 68: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 38.8661 - accuracy: 0.1708 - val_loss: 2.7758 - val_accuracy: 0.0737\n",
      "Epoch 69/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 38.7175 - accuracy: 0.1493\n",
      "Epoch 69: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 38.6140 - accuracy: 0.1526 - val_loss: 2.8263 - val_accuracy: 0.0842\n",
      "Epoch 70/400\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 38.5150 - accuracy: 0.1595\n",
      "Epoch 70: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 38.6972 - accuracy: 0.1595 - val_loss: 2.8301 - val_accuracy: 0.0737\n",
      "Epoch 71/400\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 40.2851 - accuracy: 0.1481\n",
      "Epoch 71: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 40.4545 - accuracy: 0.1412 - val_loss: 2.8680 - val_accuracy: 0.0737\n",
      "Epoch 72/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 38.6367 - accuracy: 0.1520\n",
      "Epoch 72: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 38.9337 - accuracy: 0.1481 - val_loss: 2.8446 - val_accuracy: 0.0632\n",
      "Epoch 73/400\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 41.0472 - accuracy: 0.1351\n",
      "Epoch 73: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 40.1151 - accuracy: 0.1390 - val_loss: 2.8241 - val_accuracy: 0.0737\n",
      "Epoch 74/400\n",
      "80/88 [==========================>...] - ETA: 0s - loss: 37.1821 - accuracy: 0.1900\n",
      "Epoch 74: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 37.5804 - accuracy: 0.1913 - val_loss: 2.8221 - val_accuracy: 0.0842\n",
      "Epoch 75/400\n",
      "87/88 [============================>.] - ETA: 0s - loss: 38.6218 - accuracy: 0.1701\n",
      "Epoch 75: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 38.5725 - accuracy: 0.1686 - val_loss: 2.8263 - val_accuracy: 0.0737\n",
      "Epoch 76/400\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 39.4011 - accuracy: 0.1514\n",
      "Epoch 76: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 38.9340 - accuracy: 0.1572 - val_loss: 2.8109 - val_accuracy: 0.0947\n",
      "Epoch 77/400\n",
      "79/88 [=========================>....] - ETA: 0s - loss: 38.2185 - accuracy: 0.1570\n",
      "Epoch 77: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 38.1967 - accuracy: 0.1663 - val_loss: 2.8120 - val_accuracy: 0.0526\n",
      "Epoch 78/400\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 37.8782 - accuracy: 0.1892\n",
      "Epoch 78: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 38.1693 - accuracy: 0.1800 - val_loss: 2.8704 - val_accuracy: 0.0737\n",
      "Epoch 79/400\n",
      "86/88 [============================>.] - ETA: 0s - loss: 38.6750 - accuracy: 0.1605\n",
      "Epoch 79: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 38.6254 - accuracy: 0.1595 - val_loss: 2.9167 - val_accuracy: 0.0526\n",
      "Epoch 80/400\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 39.7572 - accuracy: 0.1447\n",
      "Epoch 80: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 38.8094 - accuracy: 0.1481 - val_loss: 2.8429 - val_accuracy: 0.1053\n",
      "Epoch 81/400\n",
      "78/88 [=========================>....] - ETA: 0s - loss: 39.8255 - accuracy: 0.1564\n",
      "Epoch 81: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 39.4775 - accuracy: 0.1617 - val_loss: 2.8828 - val_accuracy: 0.0842\n",
      "Epoch 82/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 36.7485 - accuracy: 0.1959\n",
      "Epoch 82: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 36.7485 - accuracy: 0.1959 - val_loss: 2.8698 - val_accuracy: 0.0526\n",
      "Epoch 83/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 36.8977 - accuracy: 0.1708\n",
      "Epoch 83: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 36.8977 - accuracy: 0.1708 - val_loss: 2.8480 - val_accuracy: 0.0526\n",
      "Epoch 84/400\n",
      "83/88 [===========================>..] - ETA: 0s - loss: 37.3221 - accuracy: 0.1783\n",
      "Epoch 84: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 37.1074 - accuracy: 0.1868 - val_loss: 2.8314 - val_accuracy: 0.0737\n",
      "Epoch 85/400\n",
      "81/88 [==========================>...] - ETA: 0s - loss: 36.6359 - accuracy: 0.1852\n",
      "Epoch 85: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 36.0284 - accuracy: 0.1822 - val_loss: 2.8254 - val_accuracy: 0.0632\n",
      "Epoch 86/400\n",
      "87/88 [============================>.] - ETA: 0s - loss: 36.9856 - accuracy: 0.1793\n",
      "Epoch 86: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 36.9304 - accuracy: 0.1800 - val_loss: 2.8565 - val_accuracy: 0.0947\n",
      "Epoch 87/400\n",
      "78/88 [=========================>....] - ETA: 0s - loss: 36.5161 - accuracy: 0.1564\n",
      "Epoch 87: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 36.7466 - accuracy: 0.1617 - val_loss: 2.8306 - val_accuracy: 0.0632\n",
      "Epoch 88/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 38.9497 - accuracy: 0.1627\n",
      "Epoch 88: val_accuracy did not improve from 0.15789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 4ms/step - loss: 37.6119 - accuracy: 0.1503 - val_loss: 2.8885 - val_accuracy: 0.0737\n",
      "Epoch 89/400\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 37.6913 - accuracy: 0.1892\n",
      "Epoch 89: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 37.6668 - accuracy: 0.1845 - val_loss: 2.8697 - val_accuracy: 0.0632\n",
      "Epoch 90/400\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 38.2690 - accuracy: 0.1844\n",
      "Epoch 90: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 37.6114 - accuracy: 0.1891 - val_loss: 2.9225 - val_accuracy: 0.1053\n",
      "Epoch 91/400\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 37.9274 - accuracy: 0.1584\n",
      "Epoch 91: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 37.4292 - accuracy: 0.1595 - val_loss: 2.9105 - val_accuracy: 0.0632\n",
      "Epoch 92/400\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 38.3902 - accuracy: 0.1553\n",
      "Epoch 92: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 38.3613 - accuracy: 0.1549 - val_loss: 2.9130 - val_accuracy: 0.0842\n",
      "Epoch 93/400\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 36.7229 - accuracy: 0.1482\n",
      "Epoch 93: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 37.0096 - accuracy: 0.1458 - val_loss: 2.9195 - val_accuracy: 0.0737\n",
      "Epoch 94/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 37.0042 - accuracy: 0.1707\n",
      "Epoch 94: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 36.8264 - accuracy: 0.1708 - val_loss: 2.8843 - val_accuracy: 0.0842\n",
      "Epoch 95/400\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 34.6963 - accuracy: 0.1922\n",
      "Epoch 95: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34.7939 - accuracy: 0.1982 - val_loss: 2.8946 - val_accuracy: 0.0737\n",
      "Epoch 96/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 36.8280 - accuracy: 0.1600\n",
      "Epoch 96: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 37.1770 - accuracy: 0.1686 - val_loss: 2.9108 - val_accuracy: 0.0842\n",
      "Epoch 97/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 37.1893 - accuracy: 0.1707\n",
      "Epoch 97: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 37.6236 - accuracy: 0.1708 - val_loss: 2.9331 - val_accuracy: 0.0947\n",
      "Epoch 98/400\n",
      "86/88 [============================>.] - ETA: 0s - loss: 37.5138 - accuracy: 0.1907\n",
      "Epoch 98: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 37.7707 - accuracy: 0.1913 - val_loss: 2.9053 - val_accuracy: 0.1053\n",
      "Epoch 99/400\n",
      "87/88 [============================>.] - ETA: 0s - loss: 36.9057 - accuracy: 0.1770\n",
      "Epoch 99: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 36.7214 - accuracy: 0.1754 - val_loss: 2.9208 - val_accuracy: 0.0632\n",
      "Epoch 100/400\n",
      "87/88 [============================>.] - ETA: 0s - loss: 36.3276 - accuracy: 0.1816\n",
      "Epoch 100: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 36.2081 - accuracy: 0.1800 - val_loss: 2.9181 - val_accuracy: 0.0737\n",
      "Epoch 101/400\n",
      "82/88 [==========================>...] - ETA: 0s - loss: 34.7070 - accuracy: 0.1780\n",
      "Epoch 101: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 35.4202 - accuracy: 0.1845 - val_loss: 2.8872 - val_accuracy: 0.0737\n",
      "Epoch 102/400\n",
      "79/88 [=========================>....] - ETA: 0s - loss: 35.4252 - accuracy: 0.2025\n",
      "Epoch 102: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 35.2974 - accuracy: 0.1959 - val_loss: 2.8780 - val_accuracy: 0.0842\n",
      "Epoch 103/400\n",
      "79/88 [=========================>....] - ETA: 0s - loss: 35.1509 - accuracy: 0.2101\n",
      "Epoch 103: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 35.4361 - accuracy: 0.2118 - val_loss: 2.9110 - val_accuracy: 0.0947\n",
      "Epoch 104/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 33.7998 - accuracy: 0.2213\n",
      "Epoch 104: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34.9000 - accuracy: 0.2073 - val_loss: 2.9033 - val_accuracy: 0.0737\n",
      "Epoch 105/400\n",
      "78/88 [=========================>....] - ETA: 0s - loss: 34.3359 - accuracy: 0.2026\n",
      "Epoch 105: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34.4643 - accuracy: 0.1959 - val_loss: 2.9932 - val_accuracy: 0.0842\n",
      "Epoch 106/400\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 35.0069 - accuracy: 0.2026\n",
      "Epoch 106: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 35.2959 - accuracy: 0.1936 - val_loss: 2.9934 - val_accuracy: 0.0526\n",
      "Epoch 107/400\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 34.9311 - accuracy: 0.2024\n",
      "Epoch 107: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34.6072 - accuracy: 0.2050 - val_loss: 2.9750 - val_accuracy: 0.0737\n",
      "Epoch 108/400\n",
      "73/88 [=======================>......] - ETA: 0s - loss: 35.5319 - accuracy: 0.1753\n",
      "Epoch 108: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 36.3247 - accuracy: 0.1731 - val_loss: 2.9094 - val_accuracy: 0.0632\n",
      "Epoch 109/400\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 34.0275 - accuracy: 0.1838\n",
      "Epoch 109: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34.4743 - accuracy: 0.1982 - val_loss: 2.9364 - val_accuracy: 0.0947\n",
      "Epoch 110/400\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 33.9988 - accuracy: 0.2119\n",
      "Epoch 110: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 33.7456 - accuracy: 0.2164 - val_loss: 2.9190 - val_accuracy: 0.0526\n",
      "Epoch 111/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 34.1719 - accuracy: 0.2080\n",
      "Epoch 111: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34.4896 - accuracy: 0.2187 - val_loss: 2.9744 - val_accuracy: 0.0737\n",
      "Epoch 112/400\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 35.0156 - accuracy: 0.2026\n",
      "Epoch 112: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 35.5109 - accuracy: 0.1936 - val_loss: 2.9939 - val_accuracy: 0.0842\n",
      "Epoch 113/400\n",
      "87/88 [============================>.] - ETA: 0s - loss: 34.3477 - accuracy: 0.1977\n",
      "Epoch 113: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34.3047 - accuracy: 0.1982 - val_loss: 2.9425 - val_accuracy: 0.1158\n",
      "Epoch 114/400\n",
      "87/88 [============================>.] - ETA: 0s - loss: 35.1170 - accuracy: 0.2069\n",
      "Epoch 114: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 35.2029 - accuracy: 0.2050 - val_loss: 2.9501 - val_accuracy: 0.0737\n",
      "Epoch 115/400\n",
      "86/88 [============================>.] - ETA: 0s - loss: 35.7480 - accuracy: 0.1930\n",
      "Epoch 115: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 35.5763 - accuracy: 0.1936 - val_loss: 2.9857 - val_accuracy: 0.0947\n",
      "Epoch 116/400\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 34.0382 - accuracy: 0.1974\n",
      "Epoch 116: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34.0251 - accuracy: 0.1959 - val_loss: 2.9414 - val_accuracy: 0.0947\n",
      "Epoch 117/400\n",
      "79/88 [=========================>....] - ETA: 0s - loss: 35.1096 - accuracy: 0.1797\n",
      "Epoch 117: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 35.4766 - accuracy: 0.1777 - val_loss: 3.0185 - val_accuracy: 0.0947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/400\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 35.2941 - accuracy: 0.2054\n",
      "Epoch 118: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34.9038 - accuracy: 0.1959 - val_loss: 3.1159 - val_accuracy: 0.0632\n",
      "Epoch 119/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 34.9630 - accuracy: 0.2073\n",
      "Epoch 119: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34.9630 - accuracy: 0.2073 - val_loss: 3.0619 - val_accuracy: 0.0947\n",
      "Epoch 120/400\n",
      "87/88 [============================>.] - ETA: 0s - loss: 36.0432 - accuracy: 0.1540\n",
      "Epoch 120: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 35.9340 - accuracy: 0.1549 - val_loss: 2.9972 - val_accuracy: 0.0526\n",
      "Epoch 121/400\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 34.6005 - accuracy: 0.2000\n",
      "Epoch 121: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34.6925 - accuracy: 0.1936 - val_loss: 3.0030 - val_accuracy: 0.1053\n",
      "Epoch 122/400\n",
      "86/88 [============================>.] - ETA: 0s - loss: 34.4929 - accuracy: 0.2093\n",
      "Epoch 122: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34.8046 - accuracy: 0.2141 - val_loss: 3.0644 - val_accuracy: 0.1053\n",
      "Epoch 123/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 34.0453 - accuracy: 0.2187\n",
      "Epoch 123: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34.0453 - accuracy: 0.2187 - val_loss: 3.1199 - val_accuracy: 0.0632\n",
      "Epoch 124/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 35.3375 - accuracy: 0.1845\n",
      "Epoch 124: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 35.3375 - accuracy: 0.1845 - val_loss: 3.0311 - val_accuracy: 0.1053\n",
      "Epoch 125/400\n",
      "83/88 [===========================>..] - ETA: 0s - loss: 35.8805 - accuracy: 0.1759\n",
      "Epoch 125: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 35.9604 - accuracy: 0.1708 - val_loss: 2.9746 - val_accuracy: 0.0211\n",
      "Epoch 126/400\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 31.8051 - accuracy: 0.2026\n",
      "Epoch 126: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 33.3178 - accuracy: 0.2005 - val_loss: 2.9400 - val_accuracy: 0.0947\n",
      "Epoch 127/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 34.2232 - accuracy: 0.1893\n",
      "Epoch 127: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34.8295 - accuracy: 0.2005 - val_loss: 3.0292 - val_accuracy: 0.0737\n",
      "Epoch 128/400\n",
      "86/88 [============================>.] - ETA: 0s - loss: 34.4688 - accuracy: 0.2116\n",
      "Epoch 128: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34.4067 - accuracy: 0.2073 - val_loss: 2.9910 - val_accuracy: 0.0947\n",
      "Epoch 129/400\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 33.8311 - accuracy: 0.2338\n",
      "Epoch 129: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34.9196 - accuracy: 0.2278 - val_loss: 3.0430 - val_accuracy: 0.1158\n",
      "Epoch 130/400\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 33.2740 - accuracy: 0.2053\n",
      "Epoch 130: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34.1975 - accuracy: 0.1982 - val_loss: 3.0586 - val_accuracy: 0.0947\n",
      "Epoch 131/400\n",
      "87/88 [============================>.] - ETA: 0s - loss: 33.4034 - accuracy: 0.2253\n",
      "Epoch 131: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 33.3528 - accuracy: 0.2232 - val_loss: 3.0255 - val_accuracy: 0.0526\n",
      "Epoch 132/400\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 32.2988 - accuracy: 0.2243\n",
      "Epoch 132: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 33.3013 - accuracy: 0.2118 - val_loss: 2.9637 - val_accuracy: 0.0737\n",
      "Epoch 133/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 35.8455 - accuracy: 0.1959\n",
      "Epoch 133: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 35.8455 - accuracy: 0.1959 - val_loss: 3.0224 - val_accuracy: 0.0632\n",
      "Epoch 134/400\n",
      "87/88 [============================>.] - ETA: 0s - loss: 32.7613 - accuracy: 0.2046\n",
      "Epoch 134: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 32.7198 - accuracy: 0.2050 - val_loss: 3.0049 - val_accuracy: 0.0842\n",
      "Epoch 135/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 33.4623 - accuracy: 0.1891\n",
      "Epoch 135: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 33.4623 - accuracy: 0.1891 - val_loss: 3.0049 - val_accuracy: 0.0737\n",
      "Epoch 136/400\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 30.6709 - accuracy: 0.2293\n",
      "Epoch 136: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 31.5570 - accuracy: 0.2187 - val_loss: 3.0310 - val_accuracy: 0.0842\n",
      "Epoch 137/400\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 34.2527 - accuracy: 0.1711\n",
      "Epoch 137: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 33.7542 - accuracy: 0.1936 - val_loss: 3.0445 - val_accuracy: 0.0842\n",
      "Epoch 138/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 33.2619 - accuracy: 0.2255\n",
      "Epoch 138: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 33.2619 - accuracy: 0.2255 - val_loss: 3.0393 - val_accuracy: 0.0842\n",
      "Epoch 139/400\n",
      "87/88 [============================>.] - ETA: 0s - loss: 32.7127 - accuracy: 0.2115\n",
      "Epoch 139: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 32.7279 - accuracy: 0.2118 - val_loss: 2.9959 - val_accuracy: 0.0632\n",
      "Epoch 140/400\n",
      "88/88 [==============================] - ETA: 0s - loss: 34.1807 - accuracy: 0.2118\n",
      "Epoch 140: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34.1807 - accuracy: 0.2118 - val_loss: 3.0665 - val_accuracy: 0.0842\n",
      "Epoch 141/400\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 32.6379 - accuracy: 0.2400\n",
      "Epoch 141: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 32.3128 - accuracy: 0.2346 - val_loss: 2.9903 - val_accuracy: 0.0632\n",
      "Epoch 142/400\n",
      "83/88 [===========================>..] - ETA: 0s - loss: 32.8264 - accuracy: 0.2289\n",
      "Epoch 142: val_accuracy did not improve from 0.15789\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 32.5421 - accuracy: 0.2278 - val_loss: 2.9620 - val_accuracy: 0.1053\n",
      "Epoch 142: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "              batch_size=5,\n",
    "              verbose=1,\n",
    "              epochs=400,\n",
    "              validation_data = (X_val,y_val), \n",
    "              callbacks = [es,model_checkpoint]\n",
    "                    ,class_weight = {0:4.77,1:4.43,2:36.58,3:48.78,4:24.39,5:29.27,6:29.27,7:16.88,8:16.88,9:15.14,10:36.58,11:16.88,12:29.27,13:36.58,14:36.58,15:48.78,16:36.58}\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5828eaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 6ms/step - loss: 2.8017 - accuracy: 0.1596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.801692247390747, 0.1595744639635086]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = tf.keras.models.load_model('alllclassweights_march3spring24.h5')\n",
    "model1.evaluate(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ridhanya-yolo] *",
   "language": "python",
   "name": "conda-env-ridhanya-yolo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
